# -*- coding: utf-8 -*-
"""RakaminXKalbeNutritionals Data Scientist.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Gr97dCsnJqXSmByqLvPWj3K6xEL-PWzE

**IMPORT LIBRARY**
"""

! pip install pmdarima
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

from sklearn import preprocessing
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
import pmdarima as pm
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_samples, silhouette_score
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error
from yellowbrick.cluster import KElbowVisualizer

import warnings
warnings.filterwarnings('ignore')

"""**`IMPORT DATA`**"""

from google.colab import files
uploaded = files.upload()

#assign data ke variabel masing2 dan cek isinya
df_customer = pd.read_csv('Case Study - Customer.csv', sep= ';')
df_product = pd.read_csv('Case Study - Product.csv', sep= ';')
df_store = pd.read_csv('Case Study - Store.csv', sep= ';')
df_transaction = pd.read_csv('Case Study - Transaction.csv', sep= ';')

df_customer.info()
df_product.info()
df_store.info()
df_transaction.info()

"""**DATA PREPARATION**"""

df1 = pd.merge(df_transaction, df_customer, on='CustomerID', how='inner')
df2 = pd.merge(df1, df_store, on = 'StoreID', how = 'inner')
df_merged = pd.merge(df2, df_product, on = 'ProductID', how = 'inner')
df_merged.info()
df_merged.head()

#terdapat data yang jumlahnya berbeda sehingga akan disamakan dengan dihapus
df_merged.dropna(inplace = True)
df_merged.isna().sum()

#mengubah tipe data yang kurang sesuai
df_merged['Date'] = pd.to_datetime(df_merged['Date'], dayfirst=True)
df_merged.Date

"""**PREDICTIVE MODEL USING REGRESSION**"""

df_reg = df_merged.groupby('Date').agg({'Qty':'sum'})
df_reg.plot()

#membagi data ke Data Train & Data Test
print(df_reg.shape)
test_size = round(df_reg.shape[0] * 0.20)
train=df_reg.iloc[:-1*(test_size)]
test=df_reg.iloc[-1*(test_size):]
print(train.shape,test.shape)

#melakukan uji stasioner
from statsmodels.tsa.stattools import adfuller
def adf_test(dataset):
     dftest = adfuller(dataset, autolag = 'AIC')
     print("1. ADF : ",dftest[0])
     print("2. P-Value : ", dftest[1])
     print("3. Num Of Lags : ", dftest[2])
     print("4. Num Of Observations Used For ADF Regression:",      dftest[3])
     print("5. Critical Values :")
     for key, val in dftest[4].items():
         print("\t",key, ": ", val)
adf_test(df_reg)

# ACF and PACF plots to determine p and q values
fig, ax = plt.subplots(1, 2)
plot_acf(df_reg.diff().dropna(), lags=40, ax=ax[0])
plot_pacf(df_reg.diff().dropna(), lags=40, ax=ax[1])
plt.show()

#auto-fit ARIMA
auto_arima = pm.auto_arima(train, stepwise=False, seasonal=False)
auto_arima

from itertools import product
# Membuat list untuk p, d, dan q
p = range(0, 4)  # 0-3
d = range(0, 4)  # 0-3
q = range(0, 4)  # 0-3
# Menggunakan fungsi product dari itertools untuk membuat kombinasi p, d, dan q
pdq = list(product(p, d, q))
print(pdq)

from statsmodels.tsa.arima.model import ARIMA
# Membuat list untuk menyimpan Skor AIC
aic_scores = []
# Melakukan grid search manual untuk menemukan p, d, q yang optimal
for param in pdq:
    # Melakukan fitting ARIMA model
    model = ARIMA(df_reg, order=param)
    model_fit = model.fit()
    # Menambahkan aic score ke list
    aic_scores.append({'par': param, 'aic': model_fit.aic})

# Memilih aic score terkecil
best_aic = min(aic_scores, key=lambda x: x['aic'])
print(best_aic)

#Hyperparameter tuning
model_hyper = ARIMA(train, order=best_aic['par'])
model_fit_hyper = model_hyper.fit()

#Trial and error tuning
model_manual = ARIMA(train, order=(40,2,2))
model_fit_manual = model_manual.fit()

#plot forecasting
forecast_manual = model_fit_manual.forecast(len(test))
forecast_hyper = model_fit_hyper.forecast(len(test))
forecast_auto = auto_arima.predict(len(test))

df_plot = df_reg.iloc[-100:]

df_plot['forecast_manual'] = [None]*(len(df_plot)-len(forecast_manual)) + list(forecast_manual)
df_plot['forecast_hyper'] = [None]*(len(df_plot)-len(forecast_hyper)) + list(forecast_hyper)
df_plot['forecast_auto'] = [None]*(len(df_plot)-len(forecast_auto)) + list(forecast_auto)

df_plot.plot()
plt.show()

#Manual parameter tuning metrics

mae = mean_absolute_error(test, forecast_manual)
mape = mean_absolute_percentage_error(test, forecast_manual)
rmse = np.sqrt(mean_squared_error(test, forecast_manual))

print(f'mae - manual: {round(mae,4)}')
print(f'mape - manual: {round(mape,4)}')
print(f'rmse - manual: {round(rmse,4)}')

#Hyperparameter tuning metrics

mae = mean_absolute_error(test, forecast_hyper)
mape = mean_absolute_percentage_error(test, forecast_hyper)
rmse = np.sqrt(mean_squared_error(test, forecast_hyper))

print(f'mae - hyper: {round(mae,4)}')
print(f'mape - hyper: {round(mape,4)}')
print(f'rmse - hyper: {round(rmse,4)}')

#Auto-fit ARIMA metrics

mae = mean_absolute_error(test, forecast_auto)
mape = mean_absolute_percentage_error(test, forecast_auto)
rmse = np.sqrt(mean_squared_error(test, forecast_auto))

print(f'mae - auto: {round(mae,4)}')
print(f'mape - auto: {round(mape,4)}')
print(f'rmse - auto: {round(rmse,4)}')

#Apply model to forecast data
model = ARIMA(df_reg, order=(40, 2, 2))
model_fit = model.fit()
forecast = model_fit.forecast(steps=31)

forecast

#Plot forecasting
plt.figure()
plt.plot(df_reg)
plt.plot(forecast,color='orange')
plt.title('Quantity Sales Forecasting')
plt.show()

forecast.describe()

"""Dapat disimpulkan bahwa berdasarkan hasil forecast, stok persediaan harian yang perlu dipersiapkan adalah sekitar 44 buah per hari (mengambil nilai rata2)

**CLUSTERING**
"""

df_merged.head()

df_cluster = df_merged.groupby('CustomerID').agg({'TransactionID':'count',
                                                   'Qty':'sum',
                                                   'TotalAmount':'sum'}).reset_index()
df_cluster

df_cluster.info()

#Standarisasi dataset
X = df_cluster.values
X_std = StandardScaler().fit_transform(X)
df_std = pd.DataFrame(data=X_std,columns=df_cluster.columns)
df_std.isna().sum()

#Normalisasi dataset dengan minmaxscaler
X_norm = MinMaxScaler().fit_transform(X)
X_norm

# Normalisasi dataset dengan preprocessing sklearn
X_norm2 = preprocessing.normalize(df_cluster)
X_norm2

X_std

df_std

wcss= []
for n in range (1,11):
    model1 = KMeans(n_clusters=n, init='k-means++', n_init = 10, max_iter=100, tol =0.0001, random_state = 100)
    model1.fit(X_std)
    wcss.append(model1.inertia_)

plt.figure()
plt.plot(list(range(1,11)), wcss)

#Elbow Method with yellowbrick library
visualizer = KElbowVisualizer(model1, k=(2,10))
visualizer.fit(X_std)
visualizer.show()

# Kmeans n_cluster = 4
kmeans_4 = KMeans(n_clusters=4,init='k-means++',max_iter=300,n_init=10,random_state=100)
kmeans_4.fit(X_std)

# Memasukkan cluster ke dataset
df_cluster['cluster'] = kmeans_4.labels_
df_cluster.head()

plt.figure(figsize=(6,6))
sns.pairplot(data=df_cluster,hue='cluster',palette='Set1')
plt.show()

df_cluster['CustomerID'] = df_preclust['CustomerID']
df_cluster_mean = df_cluster.groupby('cluster').agg({'CustomerID':'count','TransactionID':'mean','Qty':'mean','TotalAmount':'mean'})
df_cluster_mean.sort_values('CustomerID', ascending = False)

"""Dari hasil clustering tersebut, diketahui bahwa:

*   Cluster 0 terdiri dari paling banyak customer, namun transaksi,quantity,dantotalamount yang menengah, begitu pula dengan cluster 3. Kemungkinan customer di cluster ini sudah memiliki ketertarikan untuk bertansaksi, namun mungkin perlu diberikan promosi dengan batas waktu sehingga menciptakan urgensi untuk tidak meninggalkan toko ini.
*   Cluster 1 terdiri dari 112 customer dengan transaksi,quantity,dantotalamount yang kecil. Kemungkinan customer pada segmen ini masih sering berbelanja di toko lain, sehingga perlu lebih banyak promosi untuk menarik customer di segmen ini.
*   Cluster 2 yang berwarna hijau terdiri dari customer yang lebih sedikit jumlahnya namun transaksi,quantity,dantotalamount nya paling besar. Kemungkinan segmen ini menunjukkan customer yang loyal untuk berbelanja, sehingga perlu mempertahankan kualitas layanan dan produk untuk menjaga loyalitasnya.
"""